from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
import pandas as pd
from datasets import Dataset, DatasetDict
from trl import SFTTrainer, SFTConfig
import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))
df = pd.read_csv('BindingDB_ChEMBL.tsv', sep='\t', on_bad_lines='skip', quoting=3)

def prepare_dataset(df):
    df = df.dropna(subset=['EC50 (nM)'])
    df = df[['Ligand SMILES', 'BindingDB Target Chain Sequence', 'EC50 (nM)']]
    df['EC50 (nM)'] = pd.to_numeric(df['EC50 (nM)'], errors='coerce')
    df = df[(df['EC50 (nM)'] >= 10) & (df['EC50 (nM)'] <= 100)]
    df =df.drop_duplicates(subset=['Ligand SMILES'])

    value_counts = {}
    filtered_rows = []

    for index, row in df.iterrows():
        value = row['BindingDB Target Chain Sequence']

        value_counts[value] = value_counts.get(value, 0)
        if value_counts[value] < 3:
            filtered_rows.append(row)
            value_counts[value] += 1

    final_df = pd.DataFrame(filtered_rows)

    return final_df

processed_df = prepare_dataset(df)
print(processed_df.shape)
processed_df = processed_df.rename(columns={'Ligand SMILES': 'text'})

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

def tokenize(batch):
    inputs = tokenizer(batch["text"],
        truncation=True,
        padding="max_length",
        max_length=128)
    inputs["labels"] = inputs["input_ids"].copy()

    return inputs

tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

dataset = Dataset.from_pandas(processed_df)
tokenized_data = dataset.map(tokenize, batched=True)
tokenized_data.set_format(type="torch", columns=["input_ids", "labels", "attention_mask"])

tokenizer.pad_token = "<PAD>"
model.config.pad_token_id = tokenizer.pad_token_id

train_test_split = dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

training_args = SFTConfig(
    max_seq_length= 128,
    output_dir="./drug_generator2",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=25,
    learning_rate=5e-4,
    adam_epsilon=1e-8,
    warmup_steps=100,
    save_total_limit=2,
    fp16=False,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    packing=False
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=training_args,    
)
trainer.train()

model.save_pretrained("./drug_generator_model2")
tokenizer.save_pretrained("./drug_generator_model2")

from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, PretrainedConfig, pipeline
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from trl import create_reference_model
import torch
import torch.nn as nn
from rdkit import Chem
from rdkit.Chem import rdchem
import numpy as np
import pandas as pd
import sys
from copy import deepcopy

sys.path.append('WELP-PLAPT')
from plapt import Plapt # type: ignore


df = pd.read_csv('BindingDB_ChEMBL.tsv', sep='\t', on_bad_lines='skip', quoting=3)

def prepare_dataset(df):
    df = df.dropna(subset=['EC50 (nM)'])
    df = df[['Ligand SMILES', 'BindingDB Target Chain Sequence', 'EC50 (nM)']]
    df['EC50 (nM)'] = pd.to_numeric(df['EC50 (nM)'], errors='coerce')
    df = df[(df['EC50 (nM)'] >= 10) & (df['EC50 (nM)'] <= 100)]
    df =df.drop_duplicates(subset=['Ligand SMILES'])

    value_counts = {}
    filtered_rows = []

    for index, row in df.iterrows():
        value = row['BindingDB Target Chain Sequence']

        value_counts[value] = value_counts.get(value, 0)
        if value_counts[value] < 3:
            filtered_rows.append(row)
            value_counts[value] += 1

    final_df = pd.DataFrame(filtered_rows)

    return final_df

processed_df = prepare_dataset(df)
approved_smiles = processed_df["Ligand SMILES"]
prot_seq = processed_df['BindingDB Target Chain Sequence']

class SMILESValidator:
    def __init__(self):
        self.checkers = [
            NumAtomsMolChecker(),
            ValenceErrorMolChecker(),
            ParserErrorMolChecker(),
        ]

    def validate_smiles(self, smiles):
        results = []
        for checker in self.checkers:
            if checker.check(smiles):
                results.append((checker.penalty, checker.explanation))
        return results

class NumAtomsMolChecker:
    def __init__(self):
        self.name = "num_atoms_equals_zero"
        self.explanation = "number of atoms less than 1"
        self.penalty = 8

    def check(self, smiles):
        mol = Chem.MolFromSmiles(smiles, sanitize=False)
        return mol is not None and mol.GetNumAtoms() < 2

class ValenceErrorMolChecker:
    def __init__(self):
        self.name = "valence_error"
        self.explanation = "Contains valence different from permitted"
        self.penalty = 9

    def check(self, smiles):
        try:
            mol = Chem.MolFromSmiles(smiles, sanitize=False)
            if not mol:
                return False
            Chem.SanitizeMol(mol)
            return False
        except rdchem.AtomValenceException:
            return True
        except Exception as e:
            return False

class ParserErrorMolChecker:
    def __init__(self):
        self.name = "parser_error"
        self.explanation = "Smiles could not be parsed"
        self.penalty = 10

    def check(self, smiles):
        try:
            mol = Chem.MolFromSmiles(smiles, sanitize=False)
            return mol is None
        except Exception:
            return True
       
class RewardModelConfig(PretrainedConfig):
    model_type = "reward_model"
   
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
'''
class RewardModel(PreTrainedModel):
    config_class = RewardModelConfig
   
    def __init__(self, config, approved_smiles=None, plapt_model=None, smiles_validator=None):
        super().__init__(config)
        self.config = config
        self.approved_smiles = approved_smiles if approved_smiles is not None else []
        self.plapt_model = plapt_model
        self.smiles_validator = smiles_validator
     
    def forward(self, input_ids=None, attention_mask=None, labels=None, generated_text=None, protein_sequence=None):
        if generated_text is None or protein_sequence is None:
            return torch.tensor([-1.0])
       
        rewards = []
        for text in generated_text:
            mol = Chem.MolFromSmiles(text)
            if mol is None:
                rewards.append(-1.0)
                continue
               
            validation_results = self.smiles_validator.validate_smiles(text)
            if validation_results:
                rewards.append(-1.0)
                continue
               
            if text in self.approved_smiles:
                rewards.append(-0.7)
                continue
           
            binding_affinity_results = self.plapt_model.predict_affinity(protein_sequence, text)
           
            if binding_affinity_results and isinstance(binding_affinity_results, list):

                for result in binding_affinity_results:
                    if isinstance(result, dict) and 'neg_log10_affinity_M' in result:
                        binding_affinity = result['neg_log10_affinity_M']
                        rewards.append(float(binding_affinity))
                    else:
                        rewards.append(-1.0)
            else:
                rewards.append(-1.0)
           
        return torch.tensor(rewards, dtype=torch.float32)


    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        approved_smiles = kwargs.pop('approved_smiles', [])
        plapt_model = kwargs.pop('plapt_model', None)
       
        if not args and 'config' not in kwargs:
            config = RewardModelConfig()
        else:
            config = kwargs.get('config', args[0])
           
        model = cls(config, approved_smiles=approved_smiles, plapt_model=plapt_model)
        return model
'''
class RewardModel(PreTrainedModel):
    config_class = RewardModelConfig
   
    def __init__(self, config):
        super().__init__(config)
        self.config = config
       
    def forward(self, input_ids=None, attention_mask=None, labels=None, generated_text=None, protein_sequence=None):
        if generated_text is None or protein_sequence is None:
            return torch.tensor([-1.0])
       
        rewards = []
        for text in generated_text:
            mol = Chem.MolFromSmiles(text)
            if mol is None:
                rewards.append(-1.0)
                continue
           
            rewards.append(1.0)
           
        return torch.tensor(rewards, dtype=torch.float32)

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        if not args and 'config' not in kwargs:
            config = RewardModelConfig()
        else:
            config = kwargs.get('config', args[0])
           
        model = cls(config)
        return model  
class RewardPipeline(nn.Module):
    def __init__(self, reward_model, protein_sequences):
        super().__init__()
        self.reward_model = reward_model
        self.register_buffer('current_sequence_idx', torch.tensor(0))
        self.protein_sequences = protein_sequences
       
    def forward(self, model_outputs, response_length=None):
        if isinstance(model_outputs, torch.Tensor):
            generated_text = self.reward_model.tokenizer.batch_decode(model_outputs, skip_special_tokens=True)
        else:
            generated_text = model_outputs

        current_sequence = self.protein_sequences.iloc[self.current_sequence_idx.item()]
        self.current_sequence_idx = torch.tensor((self.current_sequence_idx.item() + 1) % len(self.protein_sequences))
       
        rewards = self.reward_model(
            generated_text=generated_text,
            protein_sequence=current_sequence
        )
       
        return rewards

plapt_model = Plapt()
model = AutoModelForCausalLM.from_pretrained("./drug_generator_model2")
tokenizer = AutoTokenizer.from_pretrained("./drug_generator_model2")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

smiles_validator = SMILESValidator()
'''
config = RewardModelConfig()
reward_model = RewardModel(
    config=config,
    approved_smiles=approved_smiles,
    plapt_model=plapt_model,
    smiles_validator=smiles_validator
)
reward_model.tokenizer = tokenizer
'''

config = RewardModelConfig()
reward_model = RewardModel(config=config)
reward_model.tokenizer = tokenizer

reward_pipeline = RewardPipeline(reward_model, prot_seq)

ppo_config = PPOConfig(
    mini_batch_size= 8,
    batch_size=240,
    learning_rate=1.41e-5,
    output_dir="./ppo_drug_generator"
)

ppo_trainer = PPOTrainer(
    processing_class=tokenizer,
    model=model,
    args=ppo_config,
    ref_model=create_reference_model(model),
    reward_model=reward_pipeline,
    train_dataset=prot_seq,
    value_model=model
)

def train_ppo(epochs):
    for epoch in range(epochs):
        print(f"Starting Epoch {epoch + 1}")
       
        protein_sequences = processed_df['BindingDB Target Chain Sequence'].head(50).to_numpy()
       
        generated_smiles_dict = {seq: set() for seq in protein_sequences}
       
        for protein_sequence in protein_sequences:
            print(f"Generating SMILES for protein sequence: {protein_sequence}")
           
            while len(generated_smiles_dict[protein_sequence]) < 30:
                input_prompt = f"Protein: {protein_sequence} -> Valid SMILES:"
                inputs = tokenizer(input_prompt, return_tensors="pt", padding=True, truncation=True)
                input_ids = inputs.input_ids.to(model.device)
                attention_mask = inputs.attention_mask.to(model.device)
               
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=50,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    num_return_sequences=10,
                    bos_token_id=tokenizer.bos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id
                )
               
                generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
               
                generated_smiles = [text.split("Valid SMILES:")[-1].strip() for text in generated_texts]
               
                generated_smiles_dict[protein_sequence].update(generated_smiles)
               
                rewards = reward_pipeline(generated_smiles, [protein_sequence] * len(generated_smiles))
               
            print(f"Generated {len(generated_smiles_dict[protein_sequence])} unique SMILES for protein sequence: {protein_sequence}")
       
        print(f"Epoch {epoch + 1}: Completed generating SMILES for all 50 protein sequences.")
train_ppo(epochs=10)

model.save_pretrained("./ppo_drug_generator")
tokenizer.save_pretrained("./ppo_drug_generator")

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from rdkit import Chem

DrugGen = AutoModelForCausalLM.from_pretrained("./drug_generator_model")
DrugGen_Token = AutoTokenizer.from_pretrained("./drug_generator_model")

generator = pipeline("text-generation", model=DrugGen, tokenizer=DrugGen_Token)

protein_sequence = "MGNRSTADADGLLAGRGPAAGASAGASAGLAGQGAAALVGGVLLIGAVLAGNSLVCVSVATERALQTPTNSFIVSLAAADLLLALLVLPLFVYSEVQGGAWLLSPRLCDALMAMDVMLCTASIFNLCAISVDRFVAVAVPLRYNRQGGSRRQLLLIGATWLLSAAVAAPVLCGLNDVRGRDPAVCRLEDRDYVVYSSVCSFFLPCPLMLLLYWATFRGLQRWEVARRAKLHGRAPRRPSGPGPPSPTPPAPRLPQDPCGPDCAPPAPGLPRGPCGPDCAPAAPSLPQDPCGPDCAPPAPGLPPDPCGSNCAPPDAVRAAALPPQTPPQTRRRRRAKITGRERKAMRVLPVVVGAFLLCWTPFFVVHITQALCPACSVPPRLVSAVTWLGYVNSALNPVIYTVFNAEFRNVFRKALRACC"
input_prompt = f"Protein: {protein_sequence} -> Valid SMILES:"
result = generator(
    input_prompt,
    max_new_tokens=50,
    num_return_sequences=20,
    no_repeat_ngram_size=2,
    temperature=0.7,  
    top_k=50,
    top_p=0.95,
    do_sample=True
)

print("\nGenerated SMILES:")
for i, smile in enumerate(result):
    print(f"SMILES {i+1}: {smile['generated_text']}")




